{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag-of-words\n",
    "## The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. \n",
    "## In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "### EX :- (1) John likes to watch movies. Mary likes movies too.\n",
    "   ### BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we perform stemming on the famous speech given by Swami Vivekananda at Parliment of religion held in chicago sept 11 1893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "paragraph = \"\"\"Sisters and Brothers of America,\n",
    "It fills my heart with joy unspeakable to rise in response to the warm and cordial welcome which you have given us. \n",
    "I thank you in the name of the most ancient order of monks in the world, I thank you in the name of the mother of religions,\n",
    "and I thank you in the name of millions and millions of Hindu people of all classes and sects.\n",
    "My thanks, also, to some of the speakers on this platform who, referring to the delegates from the Orient, \n",
    "have told you that these men from far-off nations may well claim the honor of bearing to different lands the idea of toleration. \n",
    "I am proud to belong to a religion which has taught the world both tolerance and universal acceptance. We believe not only in universal toleration, but we accept all religions as true. \n",
    "I am proud to belong to a nation which has sheltered the persecuted and the refugees of all religions and all nations of the earth. \n",
    "I am proud to tell you that we have gathered in our bosom the purest remnant of the Israelites, \n",
    "who came to Southern India and took refuge with us in the very year in which their holy temple was shattered to pieces by Roman tyranny. \n",
    "I am proud to belong to the religion which has sheltered and is still fostering the remnant of the grand Zoroastrian nation. \n",
    "I will quote to you, brethren, a few lines from a hymn which I remember to have repeated from my earliest boyhood, \n",
    "which is every day repeated by millions of human beings: “As the different streams having their sources in different paths which men take through different tendencies, \n",
    "various though they appear, crooked or straight, all lead to Thee.”\n",
    "The present convention, which is one of the most august assemblies ever held,is in itself a vindication, \n",
    "a declaration to the world of the wonderful doctrine preached in the Gita: “Whosoever comes to Me, through whatsoever form, \n",
    "I reach him; all men are struggling through paths which in the end lead to me.” Sectarianism, bigotry, and its horrible descendant, \n",
    "fanaticism, have long possessed this beautiful earth. They have filled the earth with violence, drenched it often and often with human blood, destroyed civilization and sent whole nations to despair. \n",
    "Had it not been for these horrible demons, human society would be far more advanced than it is now. But their time is come; \n",
    "and I fervently hope that the bell that tolled this morning in honor of this convention may be the death-knell of all fanaticism, of all persecutions with the sword or with the pen, \n",
    "and of all uncharitable feelings between persons wending their way to the same goal.\"\"\"\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application\n",
    "### SPAM MESSAGE CLASSIFIER :-In Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail (\"ham\"). Imagine there are two literal bags full of words. One bag is filled with words found in spam messages, and the other with words found in legitimate e-mail. While any given word is likely to be somewhere in both bags, the \"spam\" bag will contain spam-related words such as \"stock\", \"Viagra\", and \"buy\" significantly more frequently, while the \"ham\" bag will contain more words related to the user's friends or workplace.\n",
    "### To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
